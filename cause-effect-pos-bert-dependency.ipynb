{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, backend as K\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Multiply, Subtract, Lambda, Input, Embedding, Dense, Activation, Lambda, Permute,Flatten, Concatenate, Dropout, GlobalAveragePooling1D, RepeatVector\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.layers import Reshape, Dot\n",
    "from tensorflow.keras.activations import softmax\n",
    "import itertools\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenization_bert import BertTokenizer\n",
    "from sklearn.metrics import ndcg_score\n",
    "import pickle, inspect\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import torch\n",
    "import re, math\n",
    "import random\n",
    "from random import sample\n",
    "from torch.autograd import Variable\n",
    "import glob\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim import SGD\n",
    "from itertools import combinations\n",
    "import modeling_bert\n",
    "from modeling_utils import (PretrainedConfig, PreTrainedModel)\n",
    "from graph_encoder import GNNRelationModel, pool\n",
    "import re\n",
    "import unicodedata\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxSeqLn = 50\n",
    "num_label = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_bracketed_text(text):\n",
    "    # Remove text enclosed in brackets using regular expressions\n",
    "    cleaned_text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean(text):\n",
    "    # Remove non-ASCII characters using the unicodedata module\n",
    "    text = text.lower()\n",
    "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
    "    text = remove_bracketed_text(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'\\s+(\\,)', r'\\1', text)\n",
    "    cleaned_text = re.sub(r'\\s+(\\.)', r'\\1', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"train.csv\", header=None)\n",
    "dftest = pd.read_csv(\"test.csv\", header=None)\n",
    "dfval = pd.read_csv(\"val.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11611\n",
      "3871\n",
      "3870\n"
     ]
    }
   ],
   "source": [
    "print(len(dftrain))\n",
    "print(len(dftest))\n",
    "print(len(dfval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '1', '1', '1', '1', '3', '3', '3', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "def find(sup, sub):\n",
    "    for i,s1 in enumerate(sup):\n",
    "        if ((len(s1) - len(sub[0])) * (len(s1) - len(sub[0])) <= 4) and (s1 in sub[0] or sub[0] in s1):\n",
    "            lsb = len(sub)\n",
    "            lsp = len(sup)\n",
    "            flag = True\n",
    "            for j in range(lsb):\n",
    "                if i+j < lsp and (sub[j] in sup[i+j] or sup[i+j] in sub[j]) and ((len(sub[j]) - len(sup[i+j])) * (len(sup[i+j]) - len(sub[j])) <= 4):\n",
    "                    continue\n",
    "                else:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag == True:\n",
    "                return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def mark_a_sentence(sent, first_entity, second_entity):\n",
    "    sent = sent.lower()\n",
    "    first_entity = first_entity.lower()\n",
    "    second_entity = second_entity.lower()\n",
    "    entity1_tokens = first_entity.split()\n",
    "    entity2_tokens = second_entity.split()\n",
    "    sent_toks = sent.split()\n",
    "    labels = ['1'] * len(sent_toks)\n",
    "    i = find(sent_toks, entity1_tokens)\n",
    "    if i > -1:\n",
    "        for j in range(i, i+len(entity1_tokens)):\n",
    "            labels[j] = '2'\n",
    "\n",
    "    i = find(sent_toks, entity2_tokens)\n",
    "    if i > -1:\n",
    "        for j in range(i, i+len(entity2_tokens)):\n",
    "            labels[j] = '3'\n",
    "    return labels\n",
    "\n",
    "first_entity = 'alvand'\n",
    "second_entity = 'the zagros mountain'\n",
    "sen = 'alvand is a subrange of the zagros mountains in western iran located south of the city of hamedan in hamedan province.'\n",
    "print(mark_a_sentence(sen, first_entity, second_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['londons', 'skyscrapers,', 'such', 'as', '30', 'st', 'mary', 'axe,', 'tower', '42,', 'the', 'broadgate', 'tower', 'and', 'one', 'canada', 'square,', 'are', 'mostly', 'in', 'the', 'two', 'financial', 'districts,', 'the', 'city', 'of', 'london', 'and', 'canary', 'wharf']\n",
      "['skyscraper']\n"
     ]
    }
   ],
   "source": [
    "first_entity = '30 st mary axe'\n",
    "second_entity = 'skyscraper'\n",
    "sen = 'london''s skyscrapers, such as 30 st mary axe, tower 42, the broadgate tower and one canada square, are mostly in the two financial districts, the city of london and canary wharf'\n",
    "print(find(sen.split(), second_entity.split()))\n",
    "\n",
    "print(sen.split())\n",
    "print(second_entity.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vals = []\n",
    "for v in dftrain.values:\n",
    "    st = v[2] \n",
    "    tokens = st.split()\n",
    "    labels = ['1'] * len(tokens)\n",
    "    first = v[0]\n",
    "    second = v[1]\n",
    "    if v[3] == False:\n",
    "        vals.append([v[0], v[1], v[2], labels, v[3]])\n",
    "    else:\n",
    "        vals.append([v[0], v[1], v[2], mark_a_sentence(v[2], v[0], v[1]), v[3]])\n",
    "        \n",
    "\n",
    "dftrain2 = pd.DataFrame(data=vals, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vals = []\n",
    "for v in dftest.values:\n",
    "    st = v[2] \n",
    "    tokens = st.split()\n",
    "    labels = ['1'] * len(tokens)\n",
    "    first = v[0]\n",
    "    second = v[1]\n",
    "    if v[3] == False:\n",
    "        vals.append([v[0], v[1], v[2], labels, v[3]])\n",
    "    else:\n",
    "        vals.append([v[0], v[1], v[2], mark_a_sentence(v[2], v[0], v[1]), v[3]])\n",
    "        \n",
    "\n",
    "dftest2 = pd.DataFrame(data=vals, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vals = []\n",
    "for v in dfval.values:\n",
    "    st = v[2] \n",
    "    tokens = st.split()\n",
    "    labels = ['1'] * len(tokens)\n",
    "    first = v[0]\n",
    "    second = v[1]\n",
    "    if v[3] == False:\n",
    "        vals.append([v[0], v[1], v[2], labels, v[3]])\n",
    "    else:\n",
    "        vals.append([v[0], v[1], v[2], mark_a_sentence(v[2], v[0], v[1]), v[3]])\n",
    "        \n",
    "\n",
    "dfval2 = pd.DataFrame(data=vals, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ami', 'val', '##o', 'che', '##le']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('ami valo chele'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 1, '2': 2, '3': 3}\n",
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "unique_labels = ['1', '2', '3']\n",
    "labels_to_ids = {k: v+1 for v, k in enumerate(unique_labels)}\n",
    "ids_to_labels = {v+1: k for v, k in enumerate(unique_labels)}\n",
    "print(labels_to_ids)\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vals = pd.read_csv(\"causality_dataset.csv\", header=None).values[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pos_tags = []\n",
    "#for v in vals:\n",
    "#    tuples = v.split('|||')\n",
    "#    for t in tuples:\n",
    "#        pos_tags.append(literal_eval(t)[2])\n",
    "#        \n",
    "#unique_pos_tags = list(set(pos_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_pos_tags_dict = {'ADP' : 1, 'SYM':2, 'INTJ':3, 'SCONJ':4, 'PRON':5, 'PUNCT':6, 'AUX':7, 'PROPN':8, 'CCONJ':9, 'NUM':10, 'X':11, 'ADJ':12, 'VERB':13, 'PART':14, 'ADV':15, 'SPACE':16, 'NOUN':17, 'DET':18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADP': 1, 'SYM': 2, 'INTJ': 3, 'SCONJ': 4, 'PRON': 5, 'PUNCT': 6, 'AUX': 7, 'PROPN': 8, 'CCONJ': 9, 'NUM': 10, 'X': 11, 'ADJ': 12, 'VERB': 13, 'PART': 14, 'ADV': 15, 'SPACE': 16, 'NOUN': 17, 'DET': 18}\n"
     ]
    }
   ],
   "source": [
    "print(unique_pos_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#text = 'a high level of cholesterol in the blood leads to an increased risk of developing coronary heart disease.'\n",
    "#dep = \"('level', 'a', 'DET')|||('level', 'high', 'ADJ')|||('leads', 'level', 'NOUN')|||('level', 'of', 'ADP')|||('of', 'cholesterol', 'NOUN')|||('level', 'in', 'ADP')|||('blood', 'the', 'DET')|||('in', 'blood', 'NOUN')|||('leads', 'leads', 'VERB')|||('leads', 'to', 'ADP')|||('risk', 'an', 'DET')|||('risk', 'increased', 'VERB')|||('to', 'risk', 'NOUN')|||('risk', 'of', 'ADP')|||('of', 'developing', 'VERB')|||('disease', 'coronary', 'ADJ')|||('disease', 'heart', 'NOUN')|||('developing', 'disease', 'NOUN')|||('leads', '.', 'PUNCT')\"\n",
    "\n",
    "#align_pos_tag(text, dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_sublist_indices(super_list, sublist):\n",
    "    indices = []\n",
    "    sublist_length = len(sublist)\n",
    "    for i in range(len(super_list) - sublist_length + 1):\n",
    "        if super_list[i:i+sublist_length] == sublist:\n",
    "            indices.extend(list(range(i, i+sublist_length)))\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#super_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "#sublist = [3, 4, 5]\n",
    "\n",
    "#indices = find_sublist_indices(super_list, sublist)\n",
    "#print(indices) [2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11611/11611 [00:17<00:00, 670.25it/s]\n"
     ]
    }
   ],
   "source": [
    "n_tr = len(dftrain)\n",
    "train_matrix = np.zeros((n_tr, maxSeqLn, maxSeqLn))\n",
    "\n",
    "for k,val in enumerate(tqdm(dftrain.values)):\n",
    "    dependencies = [literal_eval(v) for v in val[-1].split('|||')]\n",
    "    #print(dependencies)\n",
    "    bert_tokens = tokenizer.convert_ids_to_tokens(tokenizer.batch_encode_plus([val[-2]])['input_ids'][0])\n",
    "    #print(bert_tokens)\n",
    "    for dep in dependencies:\n",
    "        v1 = tokenizer.tokenize(dep[0])\n",
    "        v2 = tokenizer.tokenize(dep[1])\n",
    "        i_s = find_sublist_indices(bert_tokens, v1)\n",
    "        j_s = find_sublist_indices(bert_tokens, v2)\n",
    "        for i in i_s:\n",
    "            for j in j_s:\n",
    "                train_matrix[k][i][j] = 1.0\n",
    "    for i in range(maxSeqLn-1):\n",
    "        train_matrix[k][i][i+1] = 1.0\n",
    "        train_matrix[k][i+1][i] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3870/3870 [00:05<00:00, 651.10it/s]\n"
     ]
    }
   ],
   "source": [
    "n_vl = len(dfval)\n",
    "dev_matrix = np.zeros((n_vl, maxSeqLn, maxSeqLn))\n",
    "\n",
    "for k,val in enumerate(tqdm(dfval.values)):\n",
    "    dependencies = [literal_eval(v) for v in val[-1].split('|||')]\n",
    "    #print(dependencies)\n",
    "    bert_tokens = tokenizer.convert_ids_to_tokens(tokenizer.batch_encode_plus([val[-2]])['input_ids'][0])\n",
    "    #print(bert_tokens)\n",
    "    for dep in dependencies:\n",
    "        v1 = tokenizer.tokenize(dep[0])\n",
    "        v2 = tokenizer.tokenize(dep[1])\n",
    "        i_s = find_sublist_indices(bert_tokens, v1)\n",
    "        j_s = find_sublist_indices(bert_tokens, v2)\n",
    "        for i in i_s:\n",
    "            for j in j_s:\n",
    "                dev_matrix[k][i][j] = 1.0\n",
    "    for i in range(maxSeqLn-1):\n",
    "        dev_matrix[k][i][i+1] = 1.0\n",
    "        dev_matrix[k][i+1][i] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3871/3871 [00:05<00:00, 663.02it/s]\n"
     ]
    }
   ],
   "source": [
    "n_test = len(dftest)\n",
    "test_matrix = np.zeros((n_test, maxSeqLn, maxSeqLn))\n",
    "\n",
    "for k,val in enumerate(tqdm(dftest.values)):\n",
    "    dependencies = [literal_eval(v) for v in val[-1].split('|||')]\n",
    "    #print(dependencies)\n",
    "    bert_tokens = tokenizer.convert_ids_to_tokens(tokenizer.batch_encode_plus([val[-2]])['input_ids'][0])\n",
    "    #print(bert_tokens)\n",
    "    for dep in dependencies:\n",
    "        v1 = tokenizer.tokenize(dep[0])\n",
    "        v2 = tokenizer.tokenize(dep[1])\n",
    "        i_s = find_sublist_indices(bert_tokens, v1)\n",
    "        j_s = find_sublist_indices(bert_tokens, v2)\n",
    "        for i in i_s:\n",
    "            for j in j_s:\n",
    "                test_matrix[k][i][j] = 1.0\n",
    "    for i in range(maxSeqLn-1):\n",
    "        test_matrix[k][i][i+1] = 1.0\n",
    "        test_matrix[k][i+1][i] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_all_tokens = False\n",
    "\n",
    "\n",
    "def align_label(texts, labels):\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=maxSeqLn, truncation=True)\n",
    "    \n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(0)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else 0)\n",
    "            except:\n",
    "                label_ids.append(0)\n",
    "        previous_word_idx = word_idx\n",
    "        \n",
    "    assert len(set(label_ids)) <= 4\n",
    "    assert len(label_ids) == maxSeqLn\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "def align_pos_tag(text, dependency):\n",
    "    tokenized_inputs = tokenizer(text, padding='max_length', max_length=maxSeqLn, truncation=True)\n",
    "    \n",
    "    spacy_tokens = [literal_eval(t)[1] for t in dependency.split('|||')]\n",
    "    spacy_pos_tags = [literal_eval(t)[2] for t in dependency.split('|||')]\n",
    "    \n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    pos_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            pos_ids.append(0)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                pos_ids.append(unique_pos_tags_dict[spacy_pos_tags[word_idx]])\n",
    "            except:\n",
    "                pos_ids.append(0)\n",
    "        else:\n",
    "            try:\n",
    "                pos_ids.append(unique_pos_tags_dict[spacy_pos_tags[word_idx]] if label_all_tokens else 0)\n",
    "            except:\n",
    "                pos_ids.append(0)\n",
    "        previous_word_idx = word_idx\n",
    "        \n",
    "        \n",
    "    assert len(set(pos_ids)) <= 30\n",
    "    assert len(pos_ids) == maxSeqLn\n",
    "\n",
    "    return pos_ids\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, matrix):\n",
    "\n",
    "        lb = [i for i in df.values[:,3].tolist()]\n",
    "        txt = [i for i in df.values[:,2].tolist()]\n",
    "        deps = [i for i in df.values[:,4].tolist()]\n",
    "        \n",
    "        self.texts = txt\n",
    "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
    "        self.dependencies = deps\n",
    "        self.matrix = matrix\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "    \n",
    "    def get_batch_matrix(self, idx):\n",
    "\n",
    "        return torch.LongTensor(self.matrix[idx])\n",
    "    \n",
    "    def get_dependencies(self, idx):\n",
    "\n",
    "        return self.dependencies[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "        batch_deps = self.get_dependencies(idx)\n",
    "        batch_matrix = torch.LongTensor(self.get_batch_matrix(idx))\n",
    "\n",
    "        return batch_data, batch_labels, batch_deps, batch_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.output = modeling_bert.BertSelfOutput()\n",
    "        self.attn = BertSelfAttention()\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def forward(self, input_tensor, z, attention_mask):\n",
    "        self_outputs = self.attn(input_tensor,\n",
    "                                 z,\n",
    "                                 attention_mask)\n",
    "        attention_output = self.output(self_outputs[0],\n",
    "                                       input_tensor)\n",
    "        outputs = (attention_output,) + self_outputs[1:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FullArgSpec(args=['self', 'input_ids', 'attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'], varargs=None, varkw=None, defaults=(None, None, None, None, None, None, None, None, None, None, None, None, None), kwonlyargs=[], kwonlydefaults=None, annotations={})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('bert-uncased')\n",
    "import inspect\n",
    "inspect.getfullargspec(bert.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#AutoTokenizer.from_pretrained('/home/mkabir/my_pretrained_model/ebert')\n",
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tok = AutoTokenizer.from_pretrained('bert-uncased')\n",
    "        \n",
    "    def new_batch_encode_plus(self, texts, dependencies):\n",
    "        encoded = self.tok.batch_encode_plus(texts, max_length=maxSeqLn, truncation=True, return_tensors=\"pt\", padding='max_length')\n",
    "        encoded['tag_indices'] = torch.tensor([align_pos_tag(i,j) for i,j in zip(texts, dependencies)])\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1037,  2152,  2504,  1997, 16480,  4244, 27833,  1999,  1996,\n",
      "          2668,  5260,  2000,  2019,  3445,  3891,  1997,  4975, 21887,  2854,\n",
      "          2540,  4295,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'tag_indices': tensor([[ 0, 18, 12, 17,  1, 17,  0,  0,  1, 18, 17, 13,  1, 18, 13, 17,  1, 13,\n",
      "         12,  0, 17, 17,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])}\n"
     ]
    }
   ],
   "source": [
    "text = 'a high level of cholesterol in the blood leads to an increased risk of developing coronary heart disease.'\n",
    "dep = \"('level', 'a', 'DET')|||('level', 'high', 'ADJ')|||('leads', 'level', 'NOUN')|||('level', 'of', 'ADP')|||('of', 'cholesterol', 'NOUN')|||('level', 'in', 'ADP')|||('blood', 'the', 'DET')|||('in', 'blood', 'NOUN')|||('leads', 'leads', 'VERB')|||('leads', 'to', 'ADP')|||('risk', 'an', 'DET')|||('risk', 'increased', 'VERB')|||('to', 'risk', 'NOUN')|||('risk', 'of', 'ADP')|||('of', 'developing', 'VERB')|||('disease', 'coronary', 'ADJ')|||('disease', 'heart', 'NOUN')|||('developing', 'disease', 'NOUN')|||('leads', '.', 'PUNCT')\"\n",
    "\n",
    "tok = MyTokenizer()\n",
    "print(tok.new_batch_encode_plus([text], [dep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(maxSeqLn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombineBertModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombineBertModel, self).__init__()\n",
    "        self.syntax_encoder = GNNRelationModel().to(device)\n",
    "        self.bert = modeling_bert.BertModel()\n",
    "        self.original_bert = AutoModel.from_pretrained('bert-uncased')\n",
    "        model_dict = self.bert.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in self.original_bert.state_dict().items() if k not in 'embeddings.token_type_embeddings.weight'}\n",
    "        model_dict.update(pretrained_dict) \n",
    "        self.bert.load_state_dict(model_dict, strict=False)\n",
    "        self.bert = self.bert.to(device)\n",
    "        \n",
    "\n",
    "    def postprocess_attention_mask(self, mask):\n",
    "        mask = mask.to(dtype=next(self.parameters()).dtype) \n",
    "        mask = (1.0 - mask) * -10000.0\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, tag_ids, token_mask, adj_matrix):\n",
    "\n",
    "        sequence_output,_ = self.bert(input_ids=input_ids,attention_mask=token_mask,token_type_ids=tag_ids)\n",
    "\n",
    "        sequence_output = self.syntax_encoder(sequence_output,adj_matrix,None,None)\n",
    "\n",
    "        outputs = (sequence_output, None)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyTokenClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyTokenClassification, self).__init__()\n",
    "        self.com_bert = CombineBertModel().to(device)\n",
    "        self.tok = MyTokenizer()\n",
    "        self.num_labels = 4\n",
    "        self.classifier = nn.Linear(768,4).to(device)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "    def forward(self, txts, dependencies, adj_matrix):\n",
    "        query_tokenized = self.tok.new_batch_encode_plus(txts,dependencies).to(device)\n",
    "        token_embedding, _ = self.com_bert(query_tokenized['input_ids'], query_tokenized['tag_indices'], query_tokenized['attention_mask'], adj_matrix)\n",
    "        cls_vals = self.classifier(token_embedding)\n",
    "        probs = self.softmax(cls_vals)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = DataSequence(dftrain2, train_matrix)\n",
    "val_dataset = DataSequence(dfval2, dev_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(model):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight = torch.tensor([0.0,0.5,1.0,1.0])).to(device)\n",
    "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    tol = 3\n",
    "    patience = 0\n",
    "\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for train_text, train_label, tran_dep, adj_mat in tqdm(train_dataloader):\n",
    "            \n",
    "            train_text = list(train_text)\n",
    "            train_label = train_label.to(device)\n",
    "            tran_dep = list(tran_dep)\n",
    "            train_batch_matrix = adj_mat.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(train_text, tran_dep, train_batch_matrix).to(device)\n",
    "            batch_loss = criterion(logits.view(-1,num_label), train_label.view(-1))\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "                logits_clean = logits[i][train_label[i] != 0]\n",
    "                label_clean = train_label[i][train_label[i] != 0]\n",
    "                \n",
    "                logits_clean= logits_clean[:,1:]\n",
    "                predictions = logits_clean.argmax(dim=1) + 1\n",
    "                \n",
    "                acc = (predictions == label_clean).float().mean()\n",
    "                total_acc_train += acc\n",
    "                total_loss_train += batch_loss.item()\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_losses.append(total_loss_train)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for val_text, val_label, val_dep, adj_mat in val_dataloader:\n",
    "\n",
    "            val_text = list(val_text)\n",
    "            val_label = val_label.to(device)\n",
    "            val_batch_matrix = adj_mat.to(device)\n",
    "\n",
    "            logits = model(val_text, val_dep, val_batch_matrix).to(device)\n",
    "            batch_loss = criterion(logits.view(-1,num_label), val_label.view(-1))\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "                logits_clean = logits[i][val_label[i] != 0]\n",
    "                label_clean = val_label[i][val_label[i] != 0]\n",
    "\n",
    "                logits_clean= logits_clean[:,1:]\n",
    "                predictions = logits_clean.argmax(dim=1) + 1\n",
    "                #prec, rec, fscore, _= precision_recall_fscore_support(label_clean.cpu().detach().numpy(), predictions.cpu().detach().numpy(), average='micro')\n",
    "\n",
    "                #precisions.append(prec)\n",
    "                #recalls.append(rec)\n",
    "                #fscores.append(fscore)\n",
    "                \n",
    "                acc = (predictions == label_clean).float().mean()\n",
    "                total_acc_val += acc\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "        val_accuracy = total_acc_val / len(dfval)\n",
    "        val_loss = total_loss_val / len(dfval)\n",
    "        \n",
    "        if val_accuracy >= best_acc:\n",
    "            best_acc = val_accuracy\n",
    "            #print(best_acc)\n",
    "            patience = 0\n",
    "            torch.save(model, \"bert_pos_dep\")\n",
    "        else:\n",
    "            patience = patience + 1\n",
    "            \n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        pd.DataFrame(data=train_losses).to_csv(\"train_losses_bert_pos_dep.csv\", header=None,columns=None,index=None)\n",
    "        pd.DataFrame(data=val_losses).to_csv(\"val_losses_bert_pos_dep.csv\", header=None,columns=None,index=None)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(dftrain): .3f} | Accuracy: {total_acc_train / len(dftrain): .3f} | Val_Loss: {total_loss_val / len(dfval) : .3f} | Val Accuracy: {total_acc_val / len(dfval): .3f}')\n",
    "        \n",
    "        if patience > tol:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:42<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Loss:  1.007 | Accuracy:  0.829 | Val_Loss:  0.861 | Val Accuracy:  0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:42<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Loss:  0.868 | Accuracy:  0.910 | Val_Loss:  0.833 | Val Accuracy:  0.926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:42<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Loss:  0.846 | Accuracy:  0.926 | Val_Loss:  0.822 | Val Accuracy:  0.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:42<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Loss:  0.836 | Accuracy:  0.933 | Val_Loss:  0.817 | Val Accuracy:  0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:42<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Loss:  0.831 | Accuracy:  0.936 | Val_Loss:  0.815 | Val Accuracy:  0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 16/182 [00:04<00:38,  4.33it/s]"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "model = MyTokenClassification().to(device)\n",
    "train_loop(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = pd.read_csv(\"train_losses_bert_pos_dep.csv\").values\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9431e80748>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_losses = pd.read_csv(\"val_losses_bert_pos_dep.csv\").values\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"bert_pos_dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:09<00:00,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9735, device='cuda:0')\n",
      "0.9634316851184864\n",
      "0.9622912157366779\n",
      "0.9561336700714776\n",
      "0.8584345130457246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = DataSequence(dftest2, test_matrix)\n",
    "test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=64)\n",
    "total_acc_test = 0\n",
    "precisions = []\n",
    "recalls = []\n",
    "fscores = []\n",
    "exact_acc_ct = 0\n",
    "total_count = 0\n",
    "for test_text, test_label, test_dep, adj_mat in tqdm(test_dataloader):\n",
    "\n",
    "    test_text = list(test_text)\n",
    "    test_label = test_label.to(device)\n",
    "    test_batch_matrix = adj_mat.to(device)\n",
    "    logits = model(test_text, test_dep, test_batch_matrix).to(device)\n",
    "\n",
    "    for i in range(logits.shape[0]):\n",
    "\n",
    "        logits_clean = logits[i][test_label[i] != 0]\n",
    "        label_clean = test_label[i][test_label[i] != 0]\n",
    "\n",
    "        logits_clean= logits_clean[:,1:]\n",
    "        predictions = logits_clean.argmax(dim=1) + 1\n",
    "        prec, rec, fscore, _= precision_recall_fscore_support(label_clean.cpu().detach().numpy(), predictions.cpu().detach().numpy(), average='macro', zero_division = 1)\n",
    "\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        fscores.append(fscore)\n",
    "\n",
    "        acc = (predictions == label_clean).float().mean()\n",
    "        if acc == 1:\n",
    "            exact_acc_ct = exact_acc_ct + 1\n",
    "        total_acc_test += acc\n",
    "        total_count = total_count + 1\n",
    "\n",
    "test_accuracy = total_acc_test / len(dftest2)\n",
    "avg_prec = np.mean(precisions)\n",
    "avg_rec = np.mean(recalls)\n",
    "avg_fscores = np.mean(fscores)\n",
    "\n",
    "print(test_accuracy)\n",
    "print(avg_prec)\n",
    "print(avg_rec)\n",
    "print(avg_fscores)\n",
    "print(exact_acc_ct / total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
